{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efdd3485",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.visuals import *\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd05adef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_erdos_renyi_graphs(n_graphs, n_nodes, p):\n",
    "    \"\"\"Generate n_graphs Erdos-Renyi graphs with n_nodes and edge probability p.\"\"\"\n",
    "    graphs = []\n",
    "    for _ in range(n_graphs):\n",
    "        G = nx.erdos_renyi_graph(n_nodes, p)\n",
    "        adj_matrix = nx.to_numpy_array(G)\n",
    "        graphs.append(adj_matrix)\n",
    "    return graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43a9b9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "__N_NODES__ = 10\n",
    "__N_TRAIN__ = 300 #should be divisible by two \n",
    "\n",
    "x_train = [*generate_erdos_renyi_graphs(int(__N_TRAIN__/2), __N_NODES__, 0.4), *generate_erdos_renyi_graphs(int(__N_TRAIN__/2), __N_NODES__, 0.8)]\n",
    "x_train = np.array(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82c57713",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aldric-l/Library/Python/3.11/lib/python/site-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'vgae_1', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"/var/folders/qc/fbm2wz190nqd87d2mq7l90_80000gn/T/ipykernel_71628/922193024.py\", line 159, in train_step  *\n        total_loss, reconstruction_loss, kl_loss = vgae_loss(model, x)\n    File \"/var/folders/qc/fbm2wz190nqd87d2mq7l90_80000gn/T/ipykernel_71628/922193024.py\", line 135, in vgae_loss  *\n        reconstruction_loss = tf.nn.sigmoid_cross_entropy_with_logits(\n\n    TypeError: Input 'y' of 'Mul' Op has type float64 that does not match type float32 of argument 'x'.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 178\u001b[0m\n\u001b[1;32m    175\u001b[0m n_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_dataset:\n\u001b[0;32m--> 178\u001b[0m     total_loss, reconstruction_loss, kl_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvgae\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m     epoch_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m total_loss\n\u001b[1;32m    180\u001b[0m     epoch_train_rec_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reconstruction_loss\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/qc/fbm2wz190nqd87d2mq7l90_80000gn/T/__autograph_generated_fileyb8vl5ou.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001b[0;34m(model, x, optimizer)\u001b[0m\n\u001b[1;32m      9\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m---> 11\u001b[0m     total_loss, reconstruction_loss, kl_loss \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvgae_loss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m gradients \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tape)\u001b[38;5;241m.\u001b[39mgradient, (ag__\u001b[38;5;241m.\u001b[39mld(total_loss), ag__\u001b[38;5;241m.\u001b[39mld(model)\u001b[38;5;241m.\u001b[39mtrainable_variables), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     13\u001b[0m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(optimizer)\u001b[38;5;241m.\u001b[39mapply_gradients, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mzip\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mld(gradients), ag__\u001b[38;5;241m.\u001b[39mld(model)\u001b[38;5;241m.\u001b[39mtrainable_variables), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "File \u001b[0;32m/var/folders/qc/fbm2wz190nqd87d2mq7l90_80000gn/T/__autograph_generated_file2eu35rxy.py:14\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__vgae_loss\u001b[0;34m(model, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m n_nodes \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mshape, (ag__\u001b[38;5;241m.\u001b[39mld(x),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     13\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39meye, (ag__\u001b[38;5;241m.\u001b[39mld(n_nodes),), \u001b[38;5;28mdict\u001b[39m(batch_shape\u001b[38;5;241m=\u001b[39m[ag__\u001b[38;5;241m.\u001b[39mld(batch_size)]), fscope)\n\u001b[0;32m---> 14\u001b[0m reconstruction_loss \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigmoid_cross_entropy_with_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_reconstructed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_reconstructed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m reconstruction_loss \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(mask) \u001b[38;5;241m*\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(reconstruction_loss)\n\u001b[1;32m     16\u001b[0m reconstruction_loss \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mreduce_sum, (ag__\u001b[38;5;241m.\u001b[39mld(reconstruction_loss),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope) \u001b[38;5;241m/\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mreduce_sum, (ag__\u001b[38;5;241m.\u001b[39mld(mask),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"/var/folders/qc/fbm2wz190nqd87d2mq7l90_80000gn/T/ipykernel_71628/922193024.py\", line 159, in train_step  *\n        total_loss, reconstruction_loss, kl_loss = vgae_loss(model, x)\n    File \"/var/folders/qc/fbm2wz190nqd87d2mq7l90_80000gn/T/ipykernel_71628/922193024.py\", line 135, in vgae_loss  *\n        reconstruction_loss = tf.nn.sigmoid_cross_entropy_with_logits(\n\n    TypeError: Input 'y' of 'Mul' Op has type float64 that does not match type float32 of argument 'x'.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "# Split into train and validation sets\n",
    "train_data, val_data = train_test_split(x_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_data).batch(BATCH_SIZE)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(val_data).batch(BATCH_SIZE)\n",
    "\n",
    "# Define GCN layer\n",
    "class GraphConvLayer(layers.Layer):\n",
    "    def __init__(self, units, activation=None):\n",
    "        super(GraphConvLayer, self).__init__()\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        self.weight = None\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        features_shape = input_shape[0]\n",
    "        input_dim = features_shape[-1]\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.weight = self.add_weight(\n",
    "            shape=(input_dim, self.units),\n",
    "            initializer=\"glorot_uniform\",\n",
    "            trainable=True,\n",
    "            name=\"weight\"\n",
    "        )\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # Unpack inputs\n",
    "        features, adj_matrix = inputs\n",
    "        \n",
    "        # Add self-connections to adjacency matrix\n",
    "        batch_size = tf.shape(adj_matrix)[0]\n",
    "        n_nodes = tf.shape(adj_matrix)[1]\n",
    "        \n",
    "        # Add self-loops: A' = A + I\n",
    "        eye = tf.eye(n_nodes, batch_shape=[batch_size])\n",
    "        adj_with_self = adj_matrix + eye\n",
    "        \n",
    "        # Compute degree matrix\n",
    "        row_sum = tf.reduce_sum(adj_with_self, axis=-1)\n",
    "        deg_inv_sqrt = tf.pow(row_sum, -0.5)\n",
    "        deg_inv_sqrt = tf.where(tf.math.is_inf(deg_inv_sqrt), tf.zeros_like(deg_inv_sqrt), deg_inv_sqrt)\n",
    "        deg_inv_sqrt = tf.linalg.diag(deg_inv_sqrt)\n",
    "        \n",
    "        # Normalized adjacency: D^(-1/2) * A' * D^(-1/2)\n",
    "        norm_adj = tf.matmul(tf.matmul(deg_inv_sqrt, adj_with_self), deg_inv_sqrt)\n",
    "        \n",
    "        # Graph convolution: D^(-1/2) * A' * D^(-1/2) * X * W\n",
    "        output = tf.matmul(norm_adj, features)\n",
    "        output = tf.matmul(output, self.weight)\n",
    "        \n",
    "        # Apply activation if specified\n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "            \n",
    "        return output\n",
    "\n",
    "# Create the VGAE model with GCN layers\n",
    "class VGAE(Model):\n",
    "    def __init__(self, n_nodes, hidden_dim, latent_dim):\n",
    "        super(VGAE, self).__init__()\n",
    "        self.n_nodes = n_nodes\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Initial node features (eye matrix as one-hot encoding of nodes)\n",
    "        self.node_features = tf.Variable(\n",
    "            initial_value=tf.eye(n_nodes),\n",
    "            trainable=False,\n",
    "            dtype=tf.float32,\n",
    "            name=\"node_features\"\n",
    "        )\n",
    "        \n",
    "        # GCN Encoder layers\n",
    "        self.gc1 = GraphConvLayer(hidden_dim, activation=tf.nn.relu)\n",
    "        self.gc2_mean = GraphConvLayer(latent_dim)\n",
    "        self.gc2_logvar = GraphConvLayer(latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = layers.Dense(n_nodes, activation='sigmoid')\n",
    "        \n",
    "    def encode(self, adj):\n",
    "        # Expand node features for batch dimension\n",
    "        batch_size = tf.shape(adj)[0]\n",
    "        features = tf.tile(tf.expand_dims(self.node_features, 0), [batch_size, 1, 1])\n",
    "        \n",
    "        # First GCN layer\n",
    "        h1 = self.gc1([features, adj])\n",
    "        \n",
    "        # Second GCN layer for mean and log variance\n",
    "        z_mean = self.gc2_mean([h1, adj])\n",
    "        z_log_var = self.gc2_logvar([h1, adj])\n",
    "        \n",
    "        return z_mean, z_log_var\n",
    "    \n",
    "    def reparameterize(self, mean, log_var):\n",
    "        epsilon = tf.random.normal(shape=tf.shape(mean))\n",
    "        return mean + tf.exp(0.5 * log_var) * epsilon\n",
    "    \n",
    "    def decode(self, z):\n",
    "        # Inner product decoder\n",
    "        logits = tf.matmul(z, z, transpose_b=True)\n",
    "        # Apply sigmoid to get probabilities\n",
    "        adj_prob = tf.sigmoid(logits)\n",
    "        return adj_prob\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = self.encode(inputs)\n",
    "        z = self.reparameterize(z_mean, z_log_var)\n",
    "        reconstructed = self.decode(z)\n",
    "        return reconstructed, z_mean, z_log_var\n",
    "\n",
    "# Initialize the model\n",
    "vgae = VGAE(n_nodes=__N_NODES__, hidden_dim=HIDDEN_DIM, latent_dim=LATENT_DIM)\n",
    "\n",
    "# Define the loss function\n",
    "def vgae_loss(model, x):\n",
    "    # Forward pass\n",
    "    x_reconstructed, z_mean, z_log_var = model(x)\n",
    "    \n",
    "    # Mask for removing diagonal elements (self-loops)\n",
    "    batch_size = tf.shape(x)[0]\n",
    "    n_nodes = tf.shape(x)[1]\n",
    "    mask = 1.0 - tf.eye(n_nodes, batch_shape=[batch_size])\n",
    "    \n",
    "    # Reconstruction loss (binary cross-entropy with masking)\n",
    "    reconstruction_loss = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "        logits=tf.math.log(x_reconstructed / (1 - x_reconstructed + 1e-10)),\n",
    "        labels=x\n",
    "    )\n",
    "    reconstruction_loss = mask * reconstruction_loss\n",
    "    reconstruction_loss = tf.reduce_sum(reconstruction_loss) / tf.reduce_sum(mask)\n",
    "    \n",
    "    # KL divergence\n",
    "    kl_loss = -0.5 / (n_nodes * batch_size) * tf.reduce_sum(\n",
    "        1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
    "    )\n",
    "    \n",
    "    # Total loss\n",
    "    total_loss = reconstruction_loss + kl_loss\n",
    "    \n",
    "    return total_loss, reconstruction_loss, kl_loss\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "# Training function\n",
    "@tf.function\n",
    "def train_step(model, x, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        total_loss, reconstruction_loss, kl_loss = vgae_loss(model, x)\n",
    "    \n",
    "    gradients = tape.gradient(total_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    return total_loss, reconstruction_loss, kl_loss\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Training\n",
    "    epoch_train_loss = 0\n",
    "    epoch_train_rec_loss = 0\n",
    "    epoch_train_kl_loss = 0\n",
    "    n_batches = 0\n",
    "    \n",
    "    for batch in train_dataset:\n",
    "        total_loss, reconstruction_loss, kl_loss = train_step(vgae, batch, optimizer)\n",
    "        epoch_train_loss += total_loss\n",
    "        epoch_train_rec_loss += reconstruction_loss\n",
    "        epoch_train_kl_loss += kl_loss\n",
    "        n_batches += 1\n",
    "    \n",
    "    epoch_train_loss /= n_batches\n",
    "    epoch_train_rec_loss /= n_batches\n",
    "    epoch_train_kl_loss /= n_batches\n",
    "    \n",
    "    # Validation\n",
    "    epoch_val_loss = 0\n",
    "    epoch_val_rec_loss = 0\n",
    "    epoch_val_kl_loss = 0\n",
    "    n_batches = 0\n",
    "    \n",
    "    for batch in val_dataset:\n",
    "        total_loss, reconstruction_loss, kl_loss = vgae_loss(vgae, batch)\n",
    "        epoch_val_loss += total_loss\n",
    "        epoch_val_rec_loss += reconstruction_loss\n",
    "        epoch_val_kl_loss += kl_loss\n",
    "        n_batches += 1\n",
    "    \n",
    "    epoch_val_loss /= n_batches\n",
    "    epoch_val_rec_loss /= n_batches\n",
    "    epoch_val_kl_loss /= n_batches\n",
    "    \n",
    "    train_losses.append(epoch_train_loss.numpy())\n",
    "    val_losses.append(epoch_val_loss.numpy())\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}/{EPOCHS}, \"\n",
    "              f\"Train Loss: {epoch_train_loss:.4f}, \"\n",
    "              f\"Val Loss: {epoch_val_loss:.4f}, \"\n",
    "              f\"Train Rec Loss: {epoch_train_rec_loss:.4f}, \"\n",
    "              f\"Train KL Loss: {epoch_train_kl_loss:.4f}\")\n",
    "\n",
    "# Plot training curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('VGAE Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
