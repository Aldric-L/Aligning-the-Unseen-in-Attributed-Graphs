{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9e330c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "from framework.SMS import import_dataset_fromSMS\n",
    "import networkx as nx\n",
    "\n",
    "distance_mode = \"linear_interpolation\" # \"linear_interpolation\" or \"dijkstra\"\n",
    "\n",
    "datasetName = \"swiss1\"\n",
    "datasetSuffix = \"-500\"\n",
    "datasetPath = \"data/SMS/\" + datasetName  + datasetSuffix + \"/\"\n",
    "datasetSMS = import_dataset_fromSMS(datasetPath)\n",
    "\n",
    "CORRUPTED_NODES = 70\n",
    "\n",
    "sim = list(datasetSMS.keys())[0]\n",
    "mat = datasetSMS[sim]['adjacency_matrix']\n",
    "num_nodes = mat.shape[0]\n",
    "p_vectors_array = datasetSMS[sim]['p_array']\n",
    "dimP = p_vectors_array.shape[1]\n",
    "\n",
    "torch_points_labels=torch.tensor([0]*(num_nodes-CORRUPTED_NODES) + [1]*CORRUPTED_NODES)\n",
    "\n",
    "\n",
    "#plot_graph_from_adjacency_matrix(mat, node_color_scalars=np.sum(p_vectors_array, axis=1), cmap='plasma')\n",
    "\n",
    "def read_matrix_from_csv_loadtxt(filepath, delimiter=','):\n",
    "  \"\"\"\n",
    "  Reads a NumPy matrix from a CSV file using np.loadtxt().\n",
    "\n",
    "  Args:\n",
    "    filepath (str): The path to the CSV file.\n",
    "    delimiter (str): The character separating values in the CSV file (default is comma).\n",
    "\n",
    "  Returns:\n",
    "    numpy.ndarray: The matrix read from the CSV file.\n",
    "  \"\"\"\n",
    "  try:\n",
    "    matrix = np.loadtxt(filepath, delimiter=delimiter)\n",
    "    print(f\"Successfully loaded matrix from {filepath} using np.loadtxt().\")\n",
    "    return matrix\n",
    "  except FileNotFoundError:\n",
    "    print(f\"Error: The file '{filepath}' was not found.\")\n",
    "    return None\n",
    "  except Exception as e:\n",
    "    print(f\"An error occurred while loading the file: {e}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "path = \"data/SMS/\" + datasetName  + datasetSuffix +\"/sim_\" + str(sim)+ \"/\" + datasetName\n",
    "p_vectors_array = (read_matrix_from_csv_loadtxt(path + \"_p_matrix.csv\"))\n",
    "true_p_vectors_array = (read_matrix_from_csv_loadtxt(path + \"_true_p_matrix.csv\"))\n",
    "dimP = p_vectors_array.shape[1]\n",
    "for x in datasetSMS.values():\n",
    "    x[\"p_array\"]=p_vectors_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89daa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from framework.trainFct import *\n",
    "from torch_geometric.data import Data\n",
    "from framework.visuals import *\n",
    "from scipy.sparse.csgraph import shortest_path\n",
    "\n",
    "\n",
    "latent_dim = 2\n",
    "input_dim = dimP\n",
    "batch_size = 16\n",
    "\n",
    "encoder_hidden_dims=[128, 64, 32]\n",
    "adj_decoder_hidden_dims=[64, 64, 32]\n",
    "node_decoder_hidden_dims=[64, 64, 32]\n",
    "gcn_layers=3\n",
    "fc_layers=2\n",
    "\n",
    "dataset = []\n",
    "for x in datasetSMS.values():\n",
    "    # Create PyG data object\n",
    "    x[\"distance_matrix\"] = shortest_path(x[\"adjacency_matrix\"], directed=False, unweighted=False)\n",
    "    data = Data(x=torch.tensor(x[\"p_array\"], dtype=torch.float), \n",
    "                edge_index=adj_matrix_to_edge_index(x[\"distance_matrix\"])[0], \n",
    "                edge_labels=adj_matrix_to_edge_index(x[\"distance_matrix\"])[1],\n",
    "                adjacency_matrix=torch.tensor(x[\"distance_matrix\"]))\n",
    "    dataset.append(data)\n",
    "\n",
    "# Select a single graph to train on\n",
    "single_graph = dataset[0]\n",
    "\n",
    "# Wrap in list for compatibility with DataLoader-like expectations\n",
    "single_graph_list = [single_graph]\n",
    "\n",
    "\n",
    "dist_mat = shortest_path(x[\"distance_matrix\"], directed=False, unweighted=False)\n",
    "\n",
    "G = nx.from_numpy_array(dist_mat)\n",
    "G.remove_edges_from(nx.selfloop_edges(G)) # Remove self-loops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccff5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase1_epochs = 1500\n",
    "phase2_epochs = 200\n",
    "lr_phase1 = 0.005\n",
    "latent_dim = 2\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "encoder = MLPEncoder(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dims=[16, 16],\n",
    "    latent_dim=latent_dim,\n",
    "    mlp_layers=2,\n",
    "    dropout=0.2,\n",
    "    activation=nn.ELU()\n",
    ")\n",
    "\n",
    "\n",
    "node_decoder = NodeAttributeVariationalDecoder(\n",
    "    latent_dim=latent_dim,\n",
    "    output_dim=input_dim,\n",
    "    #hidden_dims=[5000, 128],\n",
    "    #hidden_dims=[2000, 128],\n",
    "    hidden_dims=[16],\n",
    "    dropout=0,\n",
    "    activation=nn.ELU(),\n",
    ")\n",
    "\n",
    "# Create KL annealing scheduler\n",
    "kl_scheduler = KLAnnealingScheduler(\n",
    "    anneal_start=0.0,\n",
    "    #anneal_end=0.001,\n",
    "    #anneal_end=0.8,\n",
    "    anneal_end=2,\n",
    "    anneal_steps=phase1_epochs * len(single_graph_list),\n",
    "    anneal_type='sigmoid',\n",
    ")\n",
    "\n",
    "# Create initial model with only node decoder\n",
    "model_phase1 = GraphVAE(\n",
    "    encoder=encoder,\n",
    "    decoders=[node_decoder],\n",
    "    kl_scheduler=kl_scheduler,\n",
    "    compute_latent_manifold=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ada38bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.path.exists(\"model_phase1_swissSMS.pth\"):\n",
    "    print(\"Loading pretrained model\")\n",
    "    model_phase1.load_state_dict(torch.load('model_phase1_swissSMS.pth'))\n",
    "else:\n",
    "    print(\"=== Starting Phase 1: Training encoder with node feature reconstruction ===\")\n",
    "\n",
    "    # Phase 1 training\n",
    "    history_phase1 = train_phase1(\n",
    "        model=model_phase1,\n",
    "        data_loader=single_graph_list,\n",
    "        num_epochs=phase1_epochs,\n",
    "        lr=lr_phase1,\n",
    "        weight_decay=1e-5,\n",
    "        verbose=True,\n",
    "        device=device,\n",
    "        loss_coefficient=1\n",
    "    )\n",
    "\n",
    "    print(\"\\n=== Phase 1 Complete ===\")\n",
    "\n",
    "    torch.save(model_phase1.state_dict(), 'model_phase1_swissSMS.pth')\n",
    "    print(\"\\n=== Phase 1 Saved ===\")\n",
    "\n",
    "    visualize_training(history_phase1)\n",
    "    visualize_node_features_reconstruction(model_phase1, single_graph, sample_features=dimP)\n",
    "    visualize_latent_space(model_phase1, [single_graph])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82e27f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from framework.torchVersions.distanceApproximations import DistanceApproximations\n",
    "from framework.boundedManifold import BoundedManifold\n",
    "\n",
    "model_phase1 = model_phase1.to('cpu')\n",
    "model_phase1.encoder.eval()\n",
    "model_phase2 = copy.deepcopy(model_phase1)\n",
    "\n",
    "model_phase1.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    x = single_graph.x.to(device)\n",
    "    edge_index = single_graph.edge_index.to(device)\n",
    "    latent_mu = model_phase1.encode(x, edge_index=edge_index)\n",
    "\n",
    "latent_points = latent_mu[0]\n",
    "\n",
    "model_phase1.set_compute_latent_manifold(True)\n",
    "model_phase1.construct_latent_manifold(bounds=BoundedManifold.hypercube_bounds(latent_points, margin=0.1, relative=True), force=True)\n",
    "model_phase1.get_latent_manifold().compute_full_grid_metric_tensor()\n",
    "model_phase1.get_latent_manifold().visualize_manifold_curvature(data_points=latent_points, labels=torch.tensor([0]*(num_nodes-CORRUPTED_NODES) + [1]*CORRUPTED_NODES))\n",
    "\n",
    "with torch.no_grad():\n",
    "    if distance_mode == \"linear_interpolation\":\n",
    "        dists_phase1 = model_phase1.get_latent_manifold().create_riemannian_distance_matrix(latent_points, \n",
    "                                                                                            DistanceApproximations.linear_interpolation_distance, num_points=20)\n",
    "    elif distance_mode == \"dijkstra\":\n",
    "        dists_phase1 = model_phase1.get_latent_manifold().get_grid_as_graph().compute_shortest_paths(\n",
    "                            latent_points,\n",
    "                            weight_type=\"geodesic\",  # Uses your metric tensors\n",
    "                            max_grid_neighbors=8,     # Connect to up to 8 nearest grid nodes\n",
    "                            num_threads=6\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05a055c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from framework.synthetic_manifold import get_metric\n",
    "\n",
    "true_metric = get_metric(\"soft_swiss\", seed=42)\n",
    "\n",
    "def get_theoretical_metric_tensor(point: torch.tensor):\n",
    "    return torch.tensor(true_metric(point[0], point[1], dimP))\n",
    "TheoreticalManifold = BoundedManifold(get_theoretical_metric_tensor, bounds=BoundedManifold.hypercube_bounds(torch.tensor(true_p_vectors_array), margin=0.1, relative=True))\n",
    "TheoreticalManifold.visualize_manifold_curvature(data_points=torch.tensor(true_p_vectors_array), labels=torch_points_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae3e6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from framework.synthetic_manifold import ManifoldConfig, generate_manifold_data, approx_jacobian_rank, IMMERSIONS, load_dataset, metric_pullback_alignment_error, geodesic_benchmark\n",
    "\n",
    "X, Z_true, meta, Z_vae, immersion, true_metric = load_dataset(datasetPath + \"soft_swiss\")\n",
    "\n",
    "def VAE_metric_phase1(point : np.ndarray):\n",
    "    return model_phase1.get_latent_manifold().metric_tensor(torch.tensor(point)).detach().numpy()\n",
    "\n",
    "summary = metric_pullback_alignment_error(\n",
    "    seeds_uv=true_p_vectors_array,\n",
    "    codes_z=latent_points.detach().numpy(),\n",
    "    true_metric_func=true_metric,\n",
    "    vae_metric_func=VAE_metric_phase1,\n",
    "    D=20,\n",
    "    k=12,\n",
    ")\n",
    "\n",
    "geo = geodesic_benchmark(\n",
    "    seeds_uv=true_p_vectors_array,\n",
    "    codes_z=latent_points.detach().numpy(),\n",
    "    true_metric_func=true_metric,\n",
    "    vae_metric_func=VAE_metric_phase1,\n",
    "    D=20,\n",
    "    k_graph=12,\n",
    "    subsample=500,  # speed\n",
    ")\n",
    "print(\"Geodesic distance correlation:\", geo[\"pairwise_corr\"])\n",
    "print(\"Mean pullback metric rel. error:\", summary[\"mean_rel_error\"])\n",
    "print(\"Volume element corr (sqrt(det G)):\", summary[\"volume_corr\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d48f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_p_vectors_tensor = torch.tensor(true_p_vectors_array)\n",
    "# theoretical_distances = TheoreticalManifold.create_riemannian_distance_matrix(true_p_vectors_tensor,\n",
    "#                                                                             DistanceApproximations.linear_interpolation_distance, \n",
    "#                                                                             batch_size=8, num_points=20)\n",
    "\n",
    "if distance_mode == \"linear_interpolation\":\n",
    "    theoretical_distances = TheoreticalManifold.create_riemannian_distance_matrix(true_p_vectors_tensor, \n",
    "                                                                        DistanceApproximations.linear_interpolation_distance, num_points=20)\n",
    "elif distance_mode == \"dijkstra\":\n",
    "    theoretical_distances = TheoreticalManifold.get_grid_as_graph().compute_shortest_paths(\n",
    "                        true_p_vectors_tensor,\n",
    "                        weight_type=\"geodesic\",  # Uses your metric tensors\n",
    "                        max_grid_neighbors=8,     # Connect to up to 8 nearest grid nodes\n",
    "                        num_threads=6\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a73bd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlogram(theoretical_distances/torch.max(theoretical_distances), dists_phase1/torch.max(dists_phase1),\n",
    "                titles=[\"Theoretical Distances (Linear estimation)\", \"Phase 1 Distances (Linear estimation)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8d22c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_curvature_change(model1: GraphVAE, model2: GraphVAE):\n",
    "    resolution = 30\n",
    "    bounds_np = model1.get_latent_manifold().get_bounds().cpu().numpy()\n",
    "    plot_z1 = np.linspace(bounds_np[0, 0], bounds_np[0, 1], resolution)\n",
    "    plot_z2 = np.linspace(bounds_np[1, 0], bounds_np[1, 1], resolution)\n",
    "\n",
    "    Z1_np, Z2_np = np.meshgrid(plot_z1, plot_z2)\n",
    "    Z1, Z2 = torch.from_numpy(Z1_np), torch.from_numpy(Z2_np)\n",
    "                \n",
    "    curvature_phase1 = torch.zeros((resolution, resolution))\n",
    "    curvature_phase2 = torch.zeros((resolution, resolution))\n",
    "\n",
    "    for i in range(resolution):\n",
    "        for j in range(resolution):\n",
    "            z = torch.stack([Z1[i, j], Z2[i, j]])\n",
    "            clamped_z = model1.get_latent_manifold()._clamp_point_to_bounds(z)\n",
    "            try:\n",
    "                curv_val_1 = model1.get_latent_manifold().compute_gaussian_curvature(model1.get_latent_manifold().metric_tensor(clamped_z))\n",
    "                curvature_phase1[i, j] = curv_val_1\n",
    "            except (ValueError, RuntimeError) as e:\n",
    "                print(f\"Error computing curvature at point {z}: {e}. Setting to NaN.\")\n",
    "                curvature_phase1[i, j] = torch.nan\n",
    "\n",
    "            try:\n",
    "                curv_val_2 = model2.get_latent_manifold().compute_gaussian_curvature(model2.get_latent_manifold().metric_tensor(clamped_z))\n",
    "                curvature_phase2[i, j] = curv_val_2\n",
    "            except (ValueError, RuntimeError) as e:\n",
    "                print(f\"Error computing curvature at point {z}: {e}. Setting to NaN.\")\n",
    "                curvature_phase2[i, j] = torch.nan\n",
    "            \n",
    "    curvature_diff = (curvature_phase1 - curvature_phase2)/(curvature_phase1 + 1e-8)\n",
    "    curvature_diff = curvature_diff.detach().numpy()\n",
    "\n",
    "    return curvature_diff, Z1_np, Z2_np\n",
    "\n",
    "def intermediary_diagnostics(model, data_loader, epoch):\n",
    "    with torch.no_grad():\n",
    "        model.encoder.eval()\n",
    "        x = data_loader[0].x.to(device)\n",
    "        edge_index = data_loader[0].edge_index.to(device)\n",
    "        latent_mu = model.encode(x, edge_index=edge_index)\n",
    "\n",
    "    latent_points = latent_mu[0]\n",
    "    curvature_decoder = model.get_decoder(\"adj_decoder\")\n",
    "    distances = curvature_decoder.compute_distance_matrix(latent_points)\n",
    "    L_manifold = compute_manifold_laplacian(distances=distances,\n",
    "                                            sigma=curvature_decoder.sigma_ema,\n",
    "                                            laplacian_regularization=curvature_decoder.laplacian_regularization)\n",
    "    K_manifold = compute_heat_kernel_from_laplacian(L_manifold, curvature_decoder.heat_times)\n",
    "\n",
    "    #divergence = compute_heat_kernel_divergence(K_manifold, curvature_decoder.K_graph)\n",
    "    batch_size = 5\n",
    "    diffs_mat = None\n",
    "    diffs_mat_norm = None\n",
    "    for i in range(0, len(K_manifold), batch_size):\n",
    "        batch_K = K_manifold[i:i+batch_size]\n",
    "        batch_G = curvature_decoder.K_graph[i:i+batch_size]\n",
    "        batch_times = curvature_decoder.heat_times[i:i+batch_size]\n",
    "\n",
    "        # compute differences\n",
    "        diffs = []\n",
    "        names = []\n",
    "        for M, G, t in zip(batch_K, batch_G, batch_times):\n",
    "            trace_manifold = torch.trace(M)\n",
    "            trace_graph    = torch.trace(G)\n",
    "\n",
    "            if trace_manifold > 1e-8:\n",
    "                K_manifold_norm = M * (trace_graph / trace_manifold)\n",
    "            else:\n",
    "                K_manifold_norm = M\n",
    "\n",
    "            # Compute Frobenius norm of difference\n",
    "            diff = (K_manifold_norm - G)**2\n",
    "            if diffs_mat is None:\n",
    "                diffs_mat = diff\n",
    "                diffs_mat_norm = diff / torch.max(diff)\n",
    "            else:\n",
    "                diffs_mat += diff\n",
    "                diffs_mat_norm += diff / torch.max(diff)\n",
    "            diffs.append(diff / torch.max(diff))\n",
    "            names.append(f\"Time={t:.2f}\")\n",
    "\n",
    "        plot_correlogram(*diffs, titles=names, cmap=\"gist_yarg\")\n",
    "    plot_correlogram(diffs_mat/ torch.max(diffs_mat), diffs_mat_norm/ torch.max(diffs_mat_norm), cmap=\"gist_yarg\", \n",
    "                        titles=[\"Total loss\", \"Total loss (normalized at each step)\"])\n",
    "    \n",
    "\n",
    "    curvature_diff, Z1_np, Z2_np = compute_curvature_change(model_phase1, model)\n",
    "    model.get_latent_manifold()._plot_manifold_grid(curvature_diff, Z1_np, Z2_np, latent_points=latent_points, labels=torch_points_labels, name=\"Curvature variation btwn Phases 1 and 2\")\n",
    "    #model.get_latent_manifold().visualize_manifold_curvature(data_points=latent_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83a4613",
   "metadata": {},
   "outputs": [],
   "source": [
    "from framework.KLAnnealingScheduler import NoKLScheduler\n",
    "\n",
    "lr_phase2 = 0.0005\n",
    "\n",
    "print(\"=== Starting Phase 2: Freezing encoder and adding adjacency decoder ===\")\n",
    "\n",
    "model_phase2.set_compute_latent_manifold(True)\n",
    "model_phase2.construct_latent_manifold(bounds=BoundedManifold.hypercube_bounds(latent_points, margin=0.1, relative=True), force=True)\n",
    "model_phase2.set_encoder_freeze(True)\n",
    "\n",
    "\n",
    "distance_decoder = ManifoldHeatKernelDecoder(\n",
    "    distance_mode=distance_mode,\n",
    "    latent_dim=latent_dim,\n",
    "    num_eigenvalues=500,\n",
    "    num_integration_points=20,\n",
    "    name=\"adj_decoder\",\n",
    "    ema_lag_factor=0.1,\n",
    "    num_heat_time=50,\n",
    ")\n",
    "\n",
    "# Add to your GraphVAE model\n",
    "model_phase2.add_decoder(distance_decoder)\n",
    "\n",
    "# Set reference decoder (the node attribute decoder)\n",
    "#model_phase2.get_decoder(\"adj_decoder\").giveManifoldInstance(model_phase2.get_latent_manifold())\n",
    "model_phase2.get_decoder(\"adj_decoder\").giveVAEInstance(model_phase2)\n",
    "\n",
    "# Reset KL scheduler for phase 2\n",
    "model_phase2.kl_scheduler = NoKLScheduler()\n",
    "\n",
    "# Phase 2 training\n",
    "history_phase2 = train_phase2(\n",
    "    model=model_phase2,\n",
    "    data_loader=single_graph_list,\n",
    "    latent_points=latent_points,\n",
    "    num_epochs=phase2_epochs,\n",
    "    lr=lr_phase2,\n",
    "    weight_decay=1e-5,\n",
    "#    decoder_weights={\"adj_decoder\": -1, \"node_attr_decoder\":-1 },\n",
    "    decoder_weights={\"adj_decoder\": 1, \"node_attr_decoder\":0 },\n",
    "    verbose=True,\n",
    "    device=device,\n",
    "    intermediary_diagnostics=intermediary_diagnostics,\n",
    ")\n",
    "\n",
    "print(\"\\n=== Phase 2 Complete ===\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884deeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_node_features_reconstruction(model_phase2, single_graph, sample_features=dimP)\n",
    "with torch.no_grad():\n",
    "    x = single_graph.x.to(device)\n",
    "    edge_index = single_graph.edge_index.to(device)\n",
    "    latent_mu2 = model_phase2.encode(x, edge_index=edge_index)\n",
    "\n",
    "model_phase1.get_latent_manifold().visualize_manifold_curvature(data_points=latent_mu2[0], labels=torch_points_labels)\n",
    "model_phase2.get_latent_manifold().visualize_manifold_curvature(data_points=latent_mu2[0], labels=torch_points_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ebb2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine histories\n",
    "combined_history = {\n",
    "    \"phase1\": history_phase1,\n",
    "    \"phase2\": history_phase2\n",
    "}\n",
    "visualize_training(history_phase2)\n",
    "\n",
    "merged_history = {}\n",
    "for key in history_phase1.keys():\n",
    "  if isinstance(history_phase1[key], list):\n",
    "    merged_history[key] = history_phase1[key] + history_phase2[key]\n",
    "  else:\n",
    "     merged_history[key] = dict()\n",
    "     #for key2 in history_phase1[key].keys():\n",
    "        #merged_history[key][key2] = history_phase1[key][key2] + history_phase2[key][key2]\n",
    "      \n",
    "     for key2 in history_phase2[key].keys():\n",
    "        if key2 not in history_phase1[key].keys():\n",
    "          merged_history[key][key2] = [np.nan]*len(history_phase1[\"kl_loss\"]) + history_phase2[key][key2]\n",
    "\n",
    "visualize_training(merged_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0435a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    if distance_mode == \"linear_interpolation\":\n",
    "        dists_phase2 = model_phase2.get_latent_manifold().create_riemannian_distance_matrix(latent_points, \n",
    "                                                                                            DistanceApproximations.linear_interpolation_distance, num_points=20)\n",
    "    elif distance_mode == \"dijkstra\":\n",
    "        dists_phase2 = model_phase2.get_latent_manifold().get_grid_as_graph().compute_shortest_paths(\n",
    "                            latent_points,\n",
    "                            weight_type=\"geodesic\",  # Uses your metric tensors\n",
    "                            max_grid_neighbors=8,     # Connect to up to 8 nearest grid nodes\n",
    "                            num_threads=6\n",
    "                        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d84bb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "curvature_diff, Z1_np, Z2_np = compute_curvature_change(model_phase1, model_phase2)\n",
    "model_phase2.get_latent_manifold()._plot_manifold_grid(curvature_diff, Z1_np, Z2_np, latent_points=latent_points, labels=torch_points_labels, name=\"Curvature variation btwn Phases 1 and 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a29cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(dists_phase1, torch.Tensor):\n",
    "    dists_phase1 = dists_phase1.detach().numpy()\n",
    "if isinstance(dists_phase2, torch.Tensor):\n",
    "    dists_phase2 = dists_phase2.detach().numpy()\n",
    "\n",
    "local_dists_phase1 = dists_phase1\n",
    "res = np.abs((local_dists_phase1 - dists_phase2))\n",
    "res3 = np.abs((local_dists_phase1 - dists_phase2)/local_dists_phase1)\n",
    "\n",
    "res2 = np.abs((local_dists_phase1/np.max(np.where(local_dists_phase1 > 0, local_dists_phase1, 0)) - dists_phase2/np.max(np.where(dists_phase2 > 0, dists_phase2, 0))))\n",
    "\n",
    "_ = plot_correlogram(local_dists_phase1/np.max(np.where(local_dists_phase1 > 0, local_dists_phase1, 0)), dists_phase2/np.max(np.where(dists_phase2 > 0, dists_phase2, 0)), remove_diagonal=True, triangular=True)\n",
    "\n",
    "_ = plot_correlogram((res/np.max(res))**2, (res2/np.max(res2))**2)\n",
    "_ = plot_correlogram(res3, cmap=\"inferno\", titles=[\"Relative variation of pairwise geodesic distances between phases 1 phase 2\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc3b6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6)) # Set the size of the plot for better readability\n",
    "\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['font.sans-serif'] = ['Helvetica', 'Arial', 'DejaVu Sans']\n",
    "#plt.plot(np.sum(res, axis=0)/np.max(np.sum(res, axis=0), axis=0), marker='o', linestyle='')\n",
    "#plt.plot(np.sum(res2, axis=0)/np.max(np.sum(res2, axis=0), axis=0), marker='o', linestyle='')\n",
    "plt.plot(np.sum(np.nan_to_num(res, 0.0), axis=0), marker='o', linestyle='')\n",
    "# Add a horizontal line at y=0\n",
    "\n",
    "# --- Customize the Plot ---\n",
    "#plt.savefig(\"dists_total_var_phase1_2.png\", dpi=500, bbox_inches='tight')\n",
    "\n",
    "plt.xlabel('Nodes') # Label for the x-axis\n",
    "plt.ylabel('Residuals') # Label for the y-axis, updated to reflect the ratio\n",
    "plt.title('Absolute variation at node level between phase 1 and 2 in distances') # Title of the plot, updated\n",
    "plt.grid(True, linestyle='--', alpha=0.7) # Add a grid for easier reading\n",
    "plt.tight_layout() # Adjust plot to ensure everything fits without overlapping\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9403c76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "correction_matrix = np.where(mat > 0, res, 0)\n",
    "_ = plot_correlogram(mat, correction_matrix/np.max(correction_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19186cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(res.flatten(), dist_mat.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc814c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "mutual_info_regression(res.flatten().reshape(-1, 1), dist_mat.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d84c309",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "spearmanr(res.flatten(), dist_mat.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ea4f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(res.flatten(), dist_mat.flatten(), alpha=0.6, s=10) # alpha for transparency, s for dot size\n",
    "plt.title('Scatter Plot of res vs. dist_mat', fontsize=16)\n",
    "plt.xlabel('Values from res (flattened)', fontsize=14)\n",
    "plt.ylabel('Values from dist_mat (flattened)', fontsize=14)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tight_layout() # Adjust layout to prevent labels from overlapping\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75c1c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Identify the indices for the last 70 rows/columns ---\n",
    "num_rows = res.shape[0]\n",
    "num_cols = res.shape[1]\n",
    "rows_to_color = 70\n",
    "cols_to_color = 70\n",
    "\n",
    "\n",
    "red_mask = np.zeros(res.shape, dtype=bool)\n",
    "red_mask[num_rows - rows_to_color:, num_cols - cols_to_color:] = True\n",
    "\n",
    "brown_mask = np.zeros(res.shape, dtype=bool)\n",
    "brown_mask[num_rows - rows_to_color:, :] = True\n",
    "brown_mask[:, num_cols - cols_to_color:] = True\n",
    "\n",
    "# Flatten all data\n",
    "res_flat = res.flatten()\n",
    "dist_mat_flat = dist_mat.flatten()\n",
    "red_mask_flat = red_mask.flatten()\n",
    "brown_mask_flat = brown_mask.flatten()\n",
    "\n",
    "# Separate data based on the mask\n",
    "res_red = res_flat[red_mask_flat]\n",
    "dist_mat_red = dist_mat_flat[red_mask_flat]\n",
    "res_brown = res_flat[brown_mask_flat]\n",
    "dist_mat_brown = dist_mat_flat[brown_mask_flat]\n",
    "\n",
    "res_blue = res_flat[~red_mask_flat] # Elements NOT in the red region\n",
    "dist_mat_blue = dist_mat_flat[~red_mask_flat] # Elements NOT in the red region\n",
    "\n",
    "# --- 3. Create the scatter plot with different colors ---\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# Plot the 'blue' (non-red) points first\n",
    "plt.scatter(res_blue, dist_mat_blue, alpha=0.5, s=10, color='blue', label='Other Values')\n",
    "# Plot the 'red' (last 70 rows/cols) points second, so they are on top\n",
    "plt.scatter(res_brown, dist_mat_brown, alpha=0.7, s=15, color='grey', label=f'Last {rows_to_color} rows | last {cols_to_color} cols')\n",
    "plt.scatter(res_red, dist_mat_red, alpha=0.7, s=15, color='red', label=f'Last {rows_to_color} rows & {cols_to_color} cols')\n",
    "\n",
    "plt.title('Scatter Plot of res vs. dist_mat with Highlighted Region', fontsize=16)\n",
    "plt.xlabel('Values from res (flattened)', fontsize=14)\n",
    "plt.ylabel('Values from dist_mat (flattened)', fontsize=14)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6d5531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "def keep_k_links(A, k=3, mode='or', selection='top', keep_self_loops=False, return_sparse=True):\n",
    "    \"\"\"\n",
    "    Keep only k strongest or weakest links per node in a weighted symmetric adjacency matrix.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    A : (n,n) array-like\n",
    "        Weighted adjacency matrix. Should be symmetric (function does not enforce).\n",
    "    k : int\n",
    "        Number of links to keep per node (default 3).\n",
    "    mode : {'or', 'and'}\n",
    "        'or'  -> keep edge (i,j) if i kept j OR j kept i\n",
    "        'and' -> keep edge only if i kept j AND j kept i\n",
    "    selection : {'top', 'bottom'}\n",
    "        'top'    -> keep largest weights (strongest links)\n",
    "        'bottom' -> keep smallest weights (weakest links)\n",
    "    keep_self_loops : bool\n",
    "        If False, diagonal entries are ignored/cleared before selection.\n",
    "    return_sparse : bool\n",
    "        If True, returns a scipy.sparse.csr_matrix. Otherwise, returns a dense numpy array.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A_pruned : scipy.sparse.csr_matrix or np.ndarray\n",
    "        Pruned symmetric adjacency matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    A = np.asarray(A, dtype=float)\n",
    "    n = A.shape[0]\n",
    "    if n != A.shape[1]:\n",
    "        raise ValueError(\"A must be square\")\n",
    "\n",
    "    # Ignore self-loops if requested\n",
    "    if not keep_self_loops:\n",
    "        np.fill_diagonal(A, np.inf if selection == 'bottom' else -np.inf)\n",
    "\n",
    "    k_eff = min(k, n - 1 if not keep_self_loops else n)\n",
    "    if k_eff <= 0:\n",
    "        np.fill_diagonal(A, 0.0)\n",
    "        return sp.csr_matrix((n, n)) if return_sparse else np.zeros((n, n))\n",
    "\n",
    "    # Choose argpartition direction based on selection\n",
    "    if selection == 'top':\n",
    "        idx_part = np.argpartition(-A, k_eff - 1, axis=1)[:, :k_eff]\n",
    "    elif selection == 'bottom':\n",
    "        idx_part = np.argpartition(A, k_eff - 1, axis=1)[:, :k_eff]\n",
    "    else:\n",
    "        raise ValueError(\"selection must be 'top' or 'bottom'\")\n",
    "\n",
    "    # Build mask for selected entries\n",
    "    mask = np.zeros_like(A, dtype=bool)\n",
    "    rows = np.repeat(np.arange(n)[:, None], k_eff, axis=1)\n",
    "    mask[rows, idx_part] = True\n",
    "\n",
    "    # Restore diagonal to 0 if we temporarily modified it\n",
    "    if not keep_self_loops:\n",
    "        np.fill_diagonal(A, 0.0)\n",
    "\n",
    "    # Combine masks symmetrically\n",
    "    if mode == 'or':\n",
    "        sym_mask = np.logical_or(mask, mask.T)\n",
    "    elif mode == 'and':\n",
    "        sym_mask = np.logical_and(mask, mask.T)\n",
    "    else:\n",
    "        raise ValueError(\"mode must be 'or' or 'and'\")\n",
    "\n",
    "    # Apply mask\n",
    "    pruned = np.zeros_like(A)\n",
    "    pruned[sym_mask] = A[sym_mask]\n",
    "\n",
    "    # Enforce perfect symmetry\n",
    "    pruned = np.maximum(pruned, pruned.T)\n",
    "    pruned = (pruned + pruned.T) / 2.0\n",
    "\n",
    "    if return_sparse:\n",
    "        return sp.csr_matrix(pruned)\n",
    "    else:\n",
    "        return pruned\n",
    "\n",
    "\n",
    "dist1_pruned = keep_k_links(dists_phase1, selection='bottom', k=5, mode='or', return_sparse=False)\n",
    "dist2_pruned = keep_k_links(dists_phase2, selection='bottom', k=5, mode='or', return_sparse=False)\n",
    "flop_k_dist = keep_k_links(dist_mat, selection='bottom', k=10, mode='or', return_sparse=False)\n",
    "flop_k_dist_mask = np.where(flop_k_dist > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897001da",
   "metadata": {},
   "outputs": [],
   "source": [
    "flop_k_dist_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6194017f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlogram(flop_k_dist_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14744579",
   "metadata": {},
   "outputs": [],
   "source": [
    "flop_k_dist_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1e2a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "flop_res = np.where(flop_k_dist > 0, res, 0)\n",
    "plot_correlogram(flop_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe930f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6)) # Set the size of the plot for better readability\n",
    "\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['font.sans-serif'] = ['Helvetica', 'Arial', 'DejaVu Sans']\n",
    "#plt.plot(np.sum(res, axis=0)/np.max(np.sum(res, axis=0), axis=0), marker='o', linestyle='')\n",
    "#plt.plot(np.sum(res2, axis=0)/np.max(np.sum(res2, axis=0), axis=0), marker='o', linestyle='')\n",
    "plt.plot(np.sum(np.nan_to_num(flop_res, 0.0), axis=0), marker='o', linestyle='')\n",
    "# Add a horizontal line at y=0\n",
    "\n",
    "# --- Customize the Plot ---\n",
    "#plt.savefig(\"dists_total_var_phase1_2.png\", dpi=500, bbox_inches='tight')\n",
    "\n",
    "plt.xlabel('Nodes') # Label for the x-axis\n",
    "plt.ylabel('Residuals') # Label for the y-axis, updated to reflect the ratio\n",
    "plt.title('Absolute variation at node level between phase 1 and 2 in distances') # Title of the plot, updated\n",
    "plt.grid(True, linestyle='--', alpha=0.7) # Add a grid for easier reading\n",
    "plt.tight_layout() # Adjust plot to ensure everything fits without overlapping\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744769ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "res4 = (np.where(dist1_pruned > 0, 1, 0) - np.where(dist2_pruned > 0, 1, 0))**2\n",
    "res5 = (dist1_pruned - dist2_pruned)**2\n",
    "\n",
    "plot_correlogram(res4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0ec239",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6)) # Set the size of the plot for better readability\n",
    "\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['font.sans-serif'] = ['Helvetica', 'Arial', 'DejaVu Sans']\n",
    "#plt.plot(np.sum(res, axis=0)/np.max(np.sum(res, axis=0), axis=0), marker='o', linestyle='')\n",
    "#plt.plot(np.sum(res2, axis=0)/np.max(np.sum(res2, axis=0), axis=0), marker='o', linestyle='')\n",
    "plt.plot(np.sum(np.nan_to_num(res5, 0.0), axis=0), marker='o', linestyle='')\n",
    "# Add a horizontal line at y=0\n",
    "\n",
    "# --- Customize the Plot ---\n",
    "#plt.savefig(\"dists_total_var_phase1_2.png\", dpi=500, bbox_inches='tight')\n",
    "\n",
    "plt.xlabel('Nodes') # Label for the x-axis\n",
    "plt.ylabel('Residuals') # Label for the y-axis, updated to reflect the ratio\n",
    "plt.title('Absolute variation at node level between phase 1 and 2 in distances') # Title of the plot, updated\n",
    "plt.grid(True, linestyle='--', alpha=0.7) # Add a grid for easier reading\n",
    "plt.tight_layout() # Adjust plot to ensure everything fits without overlapping\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc90f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse.csgraph import laplacian\n",
    "from scipy.linalg import eigh  # For symmetric matrices\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "from sklearn.metrics import (\n",
    "    adjusted_rand_score,\n",
    "    normalized_mutual_info_score,\n",
    "    confusion_matrix,\n",
    "    f1_score\n",
    ")\n",
    "import networkx as nx\n",
    "import community as community_louvain  # pip install python-louvain\n",
    "\n",
    "\n",
    "def evaluate_clustering(pred_labels, true_labels):\n",
    "    \"\"\"\n",
    "    Evaluate clustering predictions with several robust metrics.\n",
    "    Assumes binary classification.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure binary and integers\n",
    "    pred_labels = np.asarray(pred_labels).astype(int)\n",
    "    true_labels = np.asarray(true_labels).astype(int)\n",
    "\n",
    "    # Handle label permutation: flip if needed\n",
    "    acc1 = np.mean(pred_labels == true_labels)\n",
    "    acc2 = np.mean(1 - pred_labels == true_labels)\n",
    "    if acc2 > acc1:\n",
    "        pred_labels = 1 - pred_labels\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(true_labels, pred_labels)\n",
    "\n",
    "    # Metrics\n",
    "    ari = adjusted_rand_score(true_labels, pred_labels)\n",
    "    nmi = normalized_mutual_info_score(true_labels, pred_labels)\n",
    "    f1 = f1_score(true_labels, pred_labels)\n",
    "\n",
    "    # Report\n",
    "    print(\"Evaluation of Spectral Clustering\")\n",
    "    print(\"--------------------------------\")\n",
    "    print(f\"Accuracy               : {max(acc1, acc2):.4f}\")\n",
    "    print(f\"Adjusted Rand Index    : {ari:.4f}\")\n",
    "    print(f\"Normalized Mutual Info : {nmi:.4f}\")\n",
    "    print(f\"F1 Score               : {f1:.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": max(acc1, acc2),\n",
    "        \"ari\": ari,\n",
    "        \"nmi\": nmi,\n",
    "        \"f1\": f1,\n",
    "        \"confusion_matrix\": cm\n",
    "    }\n",
    "\n",
    "def analyze_laplacian(adj_matrix, true_labels, plot_fiedler=False):\n",
    "    \"\"\"\n",
    "    Performs eigenvalue analysis of the Laplacian matrix of a graph.\n",
    "\n",
    "    Parameters:\n",
    "    - adj_matrix: np.ndarray, symmetric adjacency matrix\n",
    "    - plot_fiedler: bool, whether to plot the Fiedler vector (2nd smallest eigenvector)\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure the matrix is symmetric\n",
    "    if not np.allclose(adj_matrix, adj_matrix.T):\n",
    "        raise ValueError(\"Adjacency matrix must be symmetric\")\n",
    "\n",
    "    # Compute unnormalized Laplacian\n",
    "    L = laplacian(adj_matrix, normed=False)\n",
    "\n",
    "    # Compute eigenvalues and eigenvectors\n",
    "    eigenvals, eigenvecs = eigh(L)\n",
    "\n",
    "    # Plot eigenvalue spectrum\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(np.sort(eigenvals), marker='o')\n",
    "    plt.title(\"Eigenvalue Spectrum of Laplacian\")\n",
    "    plt.xlabel(\"Index\")\n",
    "    plt.ylabel(\"Eigenvalue\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Fiedler vector (2nd smallest eigenvector)\n",
    "    if plot_fiedler and len(eigenvals) >= 2:\n",
    "        fiedler_vec = eigenvecs[:, 1]\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(fiedler_vec, marker='.')\n",
    "        plt.title(\"Fiedler Vector (2nd Smallest Eigenvector)\")\n",
    "        plt.xlabel(\"Node Index\")\n",
    "        plt.ylabel(\"Value\")\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        pred_labels = (fiedler_vec > 0).astype(int)\n",
    "        print(f\"Clustering accuracy (Fiedler): \")\n",
    "        evaluate_clustering(pred_labels, true_labels)\n",
    "\n",
    "    sc = SpectralClustering(n_clusters=2, affinity='precomputed', assign_labels='kmeans', random_state=0)\n",
    "    pred_labels = sc.fit_predict(adj_matrix)\n",
    "    print(f\"Clustering accuracy (SpectralClustering): \")\n",
    "    evaluate_clustering(pred_labels, true_labels)\n",
    "\n",
    "    print(f\"Clustering accuracy (Louvain): \")\n",
    "    G = nx.from_numpy_array(adj_matrix)  # Use thresholded graph here!\n",
    "    partition = community_louvain.best_partition(G)\n",
    "    pred_labels = np.array([partition[i] for i in range(len(adj_matrix))])\n",
    "    # Cluster Louvain labels into 2 meta-classes\n",
    "    kmeans = KMeans(n_clusters=2, n_init=10)\n",
    "    meta_labels = kmeans.fit_predict(pred_labels.reshape(-1, 1))\n",
    "\n",
    "    evaluate_clustering(meta_labels, true_labels)\n",
    "\n",
    "    # Useful output\n",
    "    num_components = np.sum(np.isclose(eigenvals, 0))\n",
    "    print(f\"Number of connected components (zero eigenvalues): {num_components}\")\n",
    "    print(f\"Smallest eigenvalues: {eigenvals[:5]}\")\n",
    "    return eigenvals, eigenvecs\n",
    "\n",
    "\n",
    "print(\"===\"*20)\n",
    "print(\"BASELINE\")\n",
    "eigenvals, eigenvecs = analyze_laplacian(mat, np.array([0]*(num_nodes-CORRUPTED_NODES) + [1]*CORRUPTED_NODES))\n",
    "eigenvals, eigenvecs = analyze_laplacian(dist_mat, np.array([0]*(num_nodes-CORRUPTED_NODES) + [1]*CORRUPTED_NODES))\n",
    "print(\"===\"*20)\n",
    "print(\"TEST\")\n",
    "eigenvals, eigenvecs = analyze_laplacian(res4, np.array([0]*(num_nodes-CORRUPTED_NODES) + [1]*CORRUPTED_NODES))\n",
    "eigenvals, eigenvecs = analyze_laplacian(correction_matrix, np.array([0]*(num_nodes-CORRUPTED_NODES) + [1]*CORRUPTED_NODES))\n",
    "eigenvals, eigenvecs = analyze_laplacian(flop_res, np.array([0]*(num_nodes-CORRUPTED_NODES) + [1]*CORRUPTED_NODES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93f83cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlogram(correction_matrix**2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
